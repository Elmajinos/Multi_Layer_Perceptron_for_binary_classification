ğŸ§ âœ¨ PyTorch MLP Training Project

This project demonstrates how to build and train a multilayer perceptron (MLP) in PyTorch using both a manual low-level approach and a clean high-level nn.Module implementation.

ğŸš€ Features

ğŸ”§ Manual weight initialization and updates

ğŸ§® Forward pass with ReLU & Sigmoid

ğŸ“‰ Binary classification using BCE

ğŸ” Backpropagation with loss.backward()

âš™ï¸ PyTorch autograd: leaf vs non-leaf tensors

ğŸ¤– High-level architecture using nn.Module

ğŸ“Š Training loss visualization

ğŸ“‚ Project Structure
manual_mlp.py       # Manual implementation with explicit gradient updates
module_mlp.py       # High-level MLP using nn.Module
train.py            # Training loop and loss plotting

ğŸ—ï¸ Manual MLP (Low-Level)

This version shows how neural networks work internally:

ğŸ¯ Weights created with requires_grad=True

ğŸ§  Forward propagation built manually using:

torch.mm()

F.relu()

torch.sigmoid()

ğŸ§¾ BCE loss computed manually

ğŸŸ¡ Gradients computed with Autograd

ğŸ”§ Manual weight update:

W -= lr * W.grad


This highlights key PyTorch mechanics such as gradient flow, accumulation, and numerical stability.

ğŸ§± High-Level MLP (nn.Module)

A cleaner and more scalable implementation:

class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(n_in, n_h1)
        self.fc2 = nn.Linear(n_h1, n_h2)
        self.fc3 = nn.Linear(n_h2, n_out)


Training follows the standard PyTorch loop:

optimizer.zero_grad()
loss.backward()
optimizer.step()


âœ”ï¸ Shorter
âœ”ï¸ More stable
âœ”ï¸ Easier to extend
